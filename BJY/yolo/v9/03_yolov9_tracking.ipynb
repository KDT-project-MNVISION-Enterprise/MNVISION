{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting lapx\n",
      "  Downloading lapx-0.5.9-cp38-cp38-win_amd64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\kdp\\appdata\\roaming\\python\\python38\\site-packages (from lapx) (1.24.4)\n",
      "Downloading lapx-0.5.9-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.5 MB 660.6 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 0.6/1.5 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 9.4 MB/s eta 0:00:00\n",
      "Installing collected packages: lapx\n",
      "Successfully installed lapx-0.5.9\n"
     ]
    }
   ],
   "source": [
    "# !pip install lapx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(r'./runs/detect/yolov9c_custom_1280x7203/weights/best.onnx')\n",
    "# 최소 conf 임계값 설정\n",
    "model.conf = 0.7    # 왜 안 되지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 비디오 예측\n",
    "video_file = \"./datasets/Detect_test_Cam5.mp4\"\n",
    "cap = cv2.VideoCapture(video_file)\n",
    "cap.open(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹 캠 예측\n",
    "cap = cv2.VideoCapture(0)   # 0은 기본 웹캠을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading runs\\detect\\yolov9c_custom_1280x7203\\weights\\best.onnx for ONNX Runtime inference...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnx'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  AutoUpdate skipped (offline)\n",
      "\n",
      "0: 640x640 1 Forklift, 1242.4ms\n",
      "Speed: 6.4ms preprocess, 1242.4ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Person, 4 Forklifts, 1265.7ms\n",
      "Speed: 8.1ms preprocess, 1265.7ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 Persons, 1233.2ms\n",
      "Speed: 15.1ms preprocess, 1233.2ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 Persons, 1 Forklift, 1181.8ms\n",
      "Speed: 9.5ms preprocess, 1181.8ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 Persons, 1 Trolly, 1170.7ms\n",
      "Speed: 8.3ms preprocess, 1170.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 Persons, 1 Trolly, 1149.3ms\n",
      "Speed: 8.0ms preprocess, 1149.3ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 Persons, 1 Forklift, 1153.3ms\n",
      "Speed: 10.7ms preprocess, 1153.3ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 Persons, 1 Trolly, 1251.9ms\n",
      "Speed: 8.6ms preprocess, 1251.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 Persons, 1 Trolly, 884.2ms\n",
      "Speed: 3.6ms preprocess, 884.2ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Person, 2 Forklifts, 1 Trolly, 1069.5ms\n",
      "Speed: 6.2ms preprocess, 1069.5ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 Persons, 1162.6ms\n",
      "Speed: 8.4ms preprocess, 1162.6ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 Persons, 1 Forklift, 1 Trolly, 1118.8ms\n",
      "Speed: 6.9ms preprocess, 1118.8ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 Persons, 1 Forklift, 1 Trolly, 1227.6ms\n",
      "Speed: 10.0ms preprocess, 1227.6ms inference, 16.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Person, 3 Trollys, 1234.1ms\n",
      "Speed: 15.9ms preprocess, 1234.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 Persons, 1 Trolly, 1238.2ms\n",
      "Speed: 11.1ms preprocess, 1238.2ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 Persons, 3 Trollys, 1200.7ms\n",
      "Speed: 6.3ms preprocess, 1200.7ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Person, 1 Forklift, 1 Trolly, 1464.4ms\n",
      "Speed: 9.4ms preprocess, 1464.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 Persons, 1 Forklift, 2 Trollys, 1297.0ms\n",
      "Speed: 4.6ms preprocess, 1297.0ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 Persons, 1 Forklift, 2 Trollys, 1296.5ms\n",
      "Speed: 8.6ms preprocess, 1296.5ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 Persons, 1 Trolly, 1306.2ms\n",
      "Speed: 22.7ms preprocess, 1306.2ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 Persons, 1 Forklift, 1219.2ms\n",
      "Speed: 11.2ms preprocess, 1219.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 Persons, 2 Forklifts, 1163.4ms\n",
      "Speed: 6.8ms preprocess, 1163.4ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "\n",
    "# 비디오 프레임을 반복\n",
    "while cap.isOpened():\n",
    "    # 비디오에서 프레임을 읽는다\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        \n",
    "        # 프레임에 YOLOv8 추적을 실행하여 추적을 유지\n",
    "        results = model.track(frame, persist=True)\n",
    "\n",
    "        # 결과를 프레임에 시각화합니다\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # 어노테이션된 프레임을 표시\n",
    "        cv2.imshow(\"YOLOv8 추적\", annotated_frame)\n",
    "\n",
    "        # 'q'가 눌리면 루프를 중단\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        # 비디오의 끝에 도달하면 루프를 중단\n",
    "        break\n",
    "\n",
    "# 비디오 캡처 객체를 해제하고 표시 창을 닫는다\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Results object with attributes:\n",
       "\n",
       "boxes: ultralytics.engine.results.Boxes object\n",
       "keypoints: None\n",
       "masks: None\n",
       "names: {0: 'Person', 1: 'Forklift', 2: 'Trolly', 3: 'Logo', 4: 'Door'}\n",
       "obb: None\n",
       "orig_img: array([[[ 14,  47, 127],\n",
       "        [ 24,  57, 137],\n",
       "        [ 35,  68, 146],\n",
       "        ...,\n",
       "        [ 76,  89,  78],\n",
       "        [ 78,  91,  80],\n",
       "        [ 78,  91,  80]],\n",
       "\n",
       "       [[  9,  43, 123],\n",
       "        [ 19,  53, 133],\n",
       "        [ 30,  64, 142],\n",
       "        ...,\n",
       "        [ 75,  89,  78],\n",
       "        [ 76,  90,  79],\n",
       "        [ 77,  91,  80]],\n",
       "\n",
       "       [[  5,  39, 119],\n",
       "        [ 13,  47, 127],\n",
       "        [ 22,  56, 136],\n",
       "        ...,\n",
       "        [ 75,  89,  78],\n",
       "        [ 76,  90,  79],\n",
       "        [ 77,  91,  80]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]]], dtype=uint8)\n",
       "orig_shape: (1080, 1920)\n",
       "path: 'image0.jpg'\n",
       "probs: None\n",
       "save_dir: 'runs\\\\detect\\\\track'\n",
       "speed: {'preprocess': 4.983186721801758, 'inference': 177.16503143310547, 'postprocess': 6.975889205932617}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def get_attributes_info(obj):\n",
    "    \"\"\" 객체가 갖고 있는 속성들의 종류를 리스트로 반환하는 함수 \"\"\"\n",
    "    if obj is None: return\n",
    "\n",
    "    return [attr for attr, value in inspect.getmembers(obj)\n",
    "            if not callable(value) and not attr.startswith('__')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_keys\n",
      "boxes\n",
      "keypoints\n",
      "masks\n",
      "names\n",
      "obb\n",
      "orig_img\n",
      "orig_shape\n",
      "path\n",
      "probs\n",
      "save_dir\n",
      "speed\n"
     ]
    }
   ],
   "source": [
    "temp = get_attributes_info(results[0])\n",
    "for t in temp:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([0., 0., 0.])\n",
       "conf: tensor([0.1961, 0.1486, 0.1397])\n",
       "data: tensor([[6.0072e+02, 1.1826e+02, 6.4000e+02, 2.8991e+02, 1.9610e-01, 0.0000e+00],\n",
       "        [3.9157e+02, 2.5455e+02, 5.2675e+02, 2.9995e+02, 1.4856e-01, 0.0000e+00],\n",
       "        [3.9287e+02, 2.6597e+02, 5.2423e+02, 2.9830e+02, 1.3974e-01, 0.0000e+00]])\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (480, 640)\n",
       "shape: torch.Size([3, 6])\n",
       "xywh: tensor([[620.3585, 204.0846,  39.2830, 171.6451],\n",
       "        [459.1636, 277.2474, 135.1818,  45.4038],\n",
       "        [458.5500, 282.1339, 131.3579,  32.3341]])\n",
       "xywhn: tensor([[0.9693, 0.4252, 0.0614, 0.3576],\n",
       "        [0.7174, 0.5776, 0.2112, 0.0946],\n",
       "        [0.7165, 0.5878, 0.2052, 0.0674]])\n",
       "xyxy: tensor([[600.7170, 118.2621, 640.0000, 289.9072],\n",
       "        [391.5727, 254.5455, 526.7545, 299.9493],\n",
       "        [392.8710, 265.9669, 524.2289, 298.3010]])\n",
       "xyxyn: tensor([[0.9386, 0.2464, 1.0000, 0.6040],\n",
       "        [0.6118, 0.5303, 0.8231, 0.6249],\n",
       "        [0.6139, 0.5541, 0.8191, 0.6215]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].boxes\n",
    "\n",
    "# id : 추적 ID\n",
    "# xywh : bounding box 중심점 (x,y) 와 w(가로길이), h(세로길이)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = get_attributes_info(results)\n",
    "for t in temp:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
